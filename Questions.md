# Methodological Questions
- I have noticed quotation marks being used to question the veracity of the word, i.e. "suppression" instead of suppression. How can we account for this?
- Training word embedding and transformer models: I am interested in the approach of fine-tuning models to the data but how is this done exactly? Are there any good/interesting examples of research doing this and associated github repos I could check out?
- I've been looking into the [conText](https://github.com/prodriguezsosa/conText/blob/master/vignettes/quickstart.md) R package, which "provides a fast, flexible and transparent framework to estimate context-specific word and short document embeddings using the 'a la carte' embeddings approach developed by (Khodak et al. (2018))[https://arxiv.org/abs/1805.05388] and evaluate hypotheses about covariate effects on embeddings using the regression framework developed by (Rodriguez et al. (2021))[https://github.com/prodriguezsosa/EmbeddingRegression]. A la carte embedding is an alternative to word2vec-based approaches based upon recent theoretical results for GloVe-like embeddings. The method relies mainly on a linear transformation that is efficiently learnable using pretrained word vectors and linear regression. A la carte method requires fewer examples of words in context to learn high-quality embeddings. The conText package uses 2 approaches for estimating embeddings:
1. using GloVe pre-trained embeddings and a corresponding transformation matrix to embed each document –i.e. context– ‘a la carte’. Embed a document by multiplying each of it’s feature counts with their corresponding pre-trained feature-embeddings, column-averaging the resulting vectors, and multiplying by the transformation matrix. This “transforms” a sparse V-dimensional vector (a vector of feature counts, with V = number of features in the corpus) into a dense D-dimensional vector (a D-dimensional embedding, with D = dimensions of the pre-trained embeddings). Each row in this matrix represents an ‘a la carte’ (ALC) embedding of a single instance of a feature, for example "identification”. They note that only those features that appear in the set of pre-trained embeddings will be used in computing a document’s embedding. Documents with no features overlapping with the pre-trained provided are dropped which I find concerning. Is this a thing when using pre-trained embedding models?
2. The second approach is estimating our own set of embeddings and transformation matrix, they recommend this if (a) the corpus is large enough to train a full GloVe embeddings model and (b) the corpus is distinctive in some way –e.g. a collection of articles from scientific journals. I dont think our corpus is large enough but I think its distinctive.
3. I wonder whether there could be an intermediate step where we fine-tune the pre-trained model to our data to account for all features appearing in the corpus? 

- How to best shape/structure the data for analysis? What do we understand as documents? what levels of analysis are we most interested in? Document, word, articles sorted by media ideology and congressional session? all of the above?

# Research Questions
- Which voices and narratives are amplified in media coverage? I'm thinking it could be interesting to conduct some kind of quotation analysis, using regex to identify quoted people and accompanying text. In the preliminary title analysis, I find left leaning media's discourse is dominated by Trump compared to right leaning media, which makes me think his false voter fraud narratives might have been amplified to rebute or challenge them.
- I am also interested in the kinds of evidence that are brought into the debate, this could be a statement by an expert (or not) but also statistics from an organisation, so I was thinking maybe it could be interesting to use spacy and named entity recognition to identify and count top people/institutions mentioned in coverage. My intuition is that we might find a pattern between coverage by media ideology, I expect left leaning media will bring institutions like Brennan Centre for Justice or NGO's focusing on tackling racism in their coverage of voter ID. I tried doing this but for some reason spacy's "en_core_web_sm" doesnt load. It did kind of work when I tried it on a virtual environment but couldnt process all the data through there.

